{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Car Price Prediction Project Data Collection\n",
    "\n",
    "### `Data Collection Sources:`\n",
    "\n",
    "> - Cars 24 website - https://www.cars24.com/\n",
    "> - Car Dekho website - https://www.cardekho.com/\n",
    "> - Ola Cars website - https://www.ola.cars/\n",
    "> - Olx website - https://www.olx.in/\n",
    "\n",
    "I will be scraping data from mostly these four websites (after checking the legality) for all major locations in India. Once I have the intended data I will then export them and save it in CSV format to be accessed via Microsoft Excel Program and build our **Used Car Price Prediction Project**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time, sys\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import requests\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urljoin\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the necessary libraries/dependencies here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://www.cars24.com/\"\n",
    "url2 = \"https://www.cardekho.com/\"\n",
    "url3 = \"https://www.ola.cars/\"\n",
    "url4 = \"https://www.olx.in/cars_c84\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigned `URL` into 4 different varibales for the 4 websites we are using to scrape our used car information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collecting Data from Cars 24 webpage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url1)\n",
    "print(\"Legality Response number from Cars 24 URL is:\", page) # to show the response output from the webpage\n",
    "soup = BeautifulSoup(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "driver.get(url1)\n",
    "driver.maximize_window()\n",
    "\n",
    "loc=[\"Delhi\", \"Mumbai\", \"Pune\", \"Bangalore\", \"Hyderabad\", \"Chennai\", \"Kolkata\"]\n",
    "\n",
    "for place in loc:\n",
    "    # select location manually option\n",
    "    WebDriverWait(driver,2).until(ec.element_to_be_clickable((By.XPATH,'//button[contains(text(),\"SELECT MANUALLY\")]'))).click()\n",
    "\n",
    "    # search for location\n",
    "    searchbox = WebDriverWait(driver,2).until(ec.presence_of_element_located((By.XPATH,'//div[@class=\"_6QaMX\"]/input')))\n",
    "    searchbox.send_keys(place)\n",
    "\n",
    "    # confirm click on searched location icon\n",
    "    WebDriverWait(driver,2).until(ec.element_to_be_clickable((By.XPATH,'//ul[@class=\"_16Bvy\"]/li[1]'))).click()\n",
    "\n",
    "    # choose the \"Buy Used Car\" option on the webpage\n",
    "    WebDriverWait(driver,2).until(ec.element_to_be_clickable((By.XPATH,'//a[contains(text(),\"Buy Used Car\")]'))).click()\n",
    "    \n",
    "    scroll_pause_time = 2\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        # break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break \n",
    "        \n",
    "    # fetching urls of every used car on the website\n",
    "    urls = []\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='col-4']//a\"):\n",
    "        urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='_1l4fi']//a\"):\n",
    "        urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    # obtaining the required data in the empty lists\n",
    "    location = []\n",
    "    year = []\n",
    "    kms_driven = []\n",
    "    car_model = []\n",
    "    owners = []\n",
    "    transmission = []\n",
    "    fuel_type = []\n",
    "    price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # fetching location of used cars\n",
    "    try:\n",
    "        location_tag = driver.find_element_by_xpath(\"//p[@class='_2NHUv']//span\")\n",
    "        location.append(location_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        location.append('None')\n",
    "        \n",
    "    # fetching manufacturing year of used cars\n",
    "    try:\n",
    "        year_tag = driver.find_element_by_xpath(\"//strong[@class='category']//span\")\n",
    "        year.append(year_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        year.append('None')\n",
    "        \n",
    "    # fetching number of kms driven of used cars\n",
    "    try:\n",
    "        year_tag = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        year.append(year_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        year.append('None')\n",
    "        \n",
    "    # fetching model name of used cars\n",
    "    try:\n",
    "        year_tag = driver.find_element_by_xpath(\"//a[@class='_1UhVsV']//span\")\n",
    "        year.append(year_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        year.append('None')\n",
    "        \n",
    "    # fetching owner details of used cars\n",
    "    try:\n",
    "        year_tag = driver.find_element_by_xpath(\"//span[@class='_1FH0tX']//span\")\n",
    "        year.append(year_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        year.append('None')\n",
    "        \n",
    "    # fetching transmission details of used cars\n",
    "    try:\n",
    "        year_tag = driver.find_element_by_xpath(\"//h2[@class='yhB1nd']//span\")\n",
    "        year.append(year_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        year.append('None')\n",
    "        \n",
    "    # fetching fuel type of used cars\n",
    "    try:\n",
    "        year_tag = driver.find_element_by_xpath(\"//div[@class='fMghEO']//span\")\n",
    "        year.append(year_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        year.append('None')\n",
    "        \n",
    "    # fetching price of used cars\n",
    "    try:\n",
    "        year_tag = driver.find_element_by_xpath(\"//p[@class='/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/']//span\")\n",
    "        year.append(year_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        year.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe and checking the data extracted\n",
    "cars=pd.DataFrame({'place':location,'model':brand,'year':year,'price':price,'km_driven':km,'owners':owners,\n",
    "                   'fuel':fuel,'transmission':transmission})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url2)\n",
    "print(\"Legality Response number from Car Dekho URL is:\", page) # to show the response output from the webpage\n",
    "soup = BeautifulSoup(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the legality response from the website is not 200 we cannot perform web scraping here and therefore we shall ignore the further process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url3)\n",
    "print(\"Legality Response number from Ola Cars URL is:\", page) # to show the response output from the webpage\n",
    "soup = BeautifulSoup(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here even though the legality response is 200 we are unable to inspect the webpage as right click is disabled on the website therefore web scraping in such a scenario is not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collecting Data from OLX Cars webpage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36'}\n",
    "page = requests.get(url4, headers=headers)\n",
    "print(\"Legality Response number from Olx Cars URL is:\", page) # to show the response output from the webpage\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "driver.get(url4)\n",
    "driver.maximize_window()\n",
    "\n",
    "# creating empty lists\n",
    "location = []\n",
    "year = []\n",
    "kms_driven = []\n",
    "car_model = []\n",
    "owners = []\n",
    "transmission = []\n",
    "fuel_type = []\n",
    "price = []\n",
    "\n",
    "# getting the URL of the cars\n",
    "for i in range(0,500):\n",
    "    url=driver.find_elements_by_xpath(\"//ul[@class='rl3f9 _3mXOU']/li/a\")\n",
    "    for i in url:\n",
    "        prod_URL.append(i.get_attribute('href'))\n",
    "    try:\n",
    "        next_btn=driver.find_element_by_xpath(\"//button[@class='rui-39-wj rui-3evoE rui-1JPTg']\").click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "for i in prod_URL:\n",
    "    driver.get(i)    \n",
    "\n",
    "    #Extracting car name\n",
    "    try:\n",
    "        brand.append(driver.find_element_by_xpath(\"//div[@class='_3_knn']/div/span[2]\").text)\n",
    "    except NoSuchElementException as e:\n",
    "        brand.append(\"-\")\n",
    "        \n",
    "    #Extracting the year\n",
    "    try:\n",
    "        year.append(driver.find_element_by_xpath(\"//span[@data-aut-id='value_year']\").text)\n",
    "    except NoSuchElementException as e:\n",
    "        year.append(\"-\")\n",
    "        \n",
    "    #Extracting the fuel consumed\n",
    "    try:\n",
    "        fuel.append(driver.find_element_by_xpath(\"//span[@data-aut-id='value_petrol']\").text)\n",
    "    except NoSuchElementException as e:\n",
    "        fuel.append(\"-\")  \n",
    "        \n",
    "    #Extracting the transmission\n",
    "    try:\n",
    "        transmission.append(driver.find_element_by_xpath(\"//span[@data-aut-id='value_transmission']\").text)\n",
    "    except NoSuchElementException as e:\n",
    "        transmission.append(\"-\") \n",
    "        \n",
    "    #Extracting km driven\n",
    "    try:\n",
    "        km.append(driver.find_element_by_xpath(\"//span[@data-aut-id='value_mileage']\").text)\n",
    "    except NoSuchElementException as e:\n",
    "        km.append(\"-\") \n",
    "        \n",
    "    #Extracting the price details\n",
    "    try:\n",
    "        price.append(driver.find_element_by_xpath(\"//span[@data-aut-id='itemPrice']\").text.replace('â‚¹',''))\n",
    "    except NoSuchElementException as e:\n",
    "        price.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe and checking the data extracted\n",
    "cars=pd.DataFrame({'place':location,'model':brand,'year':year,'price':price,'km_driven':km,'owners':owners,\n",
    "                   'fuel':fuel,'transmission':transmission})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.to_csv(\"Used_Car_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exported the collected used car details from dataframe to csv format to be used for machine learning model building.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
